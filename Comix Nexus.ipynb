{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "def get_category_links(section_url):\n",
    "    html = urlopen(section_url).read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    boccat = soup.find(\"dl\", \"boccat\")\n",
    "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
    "    return category_links\n",
    "'''\n",
    "\n",
    "\"\"\"index = 'https://en.wikipedia.org/wiki/Watchmen' #'http://www.snackdata.com'\n",
    "soup = BeautifulSoup(requests.get(index).content, 'html.parser')\n",
    "#items = soup.find(id='indexholder').find_all('li')\n",
    "soup.find(id='Story')\n",
    "#[index + item.find('a')['href'] for item in items]\n",
    "\"\"\"\n",
    "#import requests\n",
    "#BeautifulSoup(requests.get(links[0]).content, 'html.parser')\n",
    "\n",
    "\"\"\"\n",
    "section_url = 'https://en.wikipedia.org/wiki/Watchmen'\n",
    "html = urlopen(section_url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "watch = ''.join(str(soup.findAll('p')[17:22]))\n",
    "\n",
    "section_url = 'https://en.wikipedia.org/wiki/Saga_(comic_book)'\n",
    "html = urlopen(section_url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "saga = ''.join(str(soup.findAll('p')[16:20]))\n",
    "\n",
    "section_url = 'https://en.wikipedia.org/wiki/V_for_Vendetta'\n",
    "html = urlopen(section_url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "v = ''.join(str(soup.findAll('p')[15:22]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scraping\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "\n",
    "def get_text(link_list):\n",
    "    links = []\n",
    "    text_list = list()\n",
    "    for link in link_list:\n",
    "        try:\n",
    "            html = urlopen(link).read()\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            text_list.append(''.join(str(soup.findAll('p'))))\n",
    "            links.append(link)\n",
    "        except:\n",
    "            print link           \n",
    "    return (links, text_list)\n",
    "\n",
    "\n",
    "links = ['https://en.wikipedia.org/wiki/100_Bullets',\n",
    "        'https://en.wikipedia.org/wiki/2000_AD_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/300_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/A_Contract_with_God',\n",
    "        'https://en.wikipedia.org/wiki/Akira_(manga)',\n",
    "        'https://en.wikipedia.org/wiki/All-Star_Superman',\n",
    "        'https://en.wikipedia.org/wiki/Annihilation_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Arkham_Asylum:_A_Serious_House_on_Serious_Earth',\n",
    "        'https://en.wikipedia.org/wiki/Astonishing_X-Men,\n",
    "        'https://en.wikipedia.org/wiki/Black_Hole_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Aya_of_Yop_City',\n",
    "        'https://en.wikipedia.org/wiki/Barefoot_Gen',\n",
    "        'https://en.wikipedia.org/wiki/Batman:_The_Killing_Joke',\n",
    "        'https://en.wikipedia.org/wiki/Batman:_The_Long_Halloween',\n",
    "        'https://en.wikipedia.org/wiki/Batman:_Year_One',\n",
    "        'https://en.wikipedia.org/wiki/Blankets_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Bone_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Born_Again_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Chew_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Civil_War_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/DMZ_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Ex_Machina_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Fables_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/From_Hell',\n",
    "        'https://en.wikipedia.org/wiki/Fun_Home',\n",
    "        'https://en.wikipedia.org/wiki/Ghost_World',\n",
    "        'https://en.wikipedia.org/wiki/Girl_Genius',\n",
    "        'https://en.wikipedia.org/wiki/Hellblazer',\n",
    "        'https://en.wikipedia.org/wiki/Hellboy:_Seed_of_Destruction',\n",
    "        'https://en.wikipedia.org/wiki/Kingdom_Come_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Krazy_Kat',\n",
    "        'https://en.wikipedia.org/wiki/List_of_Criminal_story_arcs',\n",
    "        'https://en.wikipedia.org/wiki/List_of_Preacher_story_arcs',\n",
    "        'https://en.wikipedia.org/wiki/List_of_Y:_The_Last_Man_story_arcs',\n",
    "        'https://en.wikipedia.org/wiki/Locke_%26_Key',\n",
    "        'https://en.wikipedia.org/wiki/Lone_Wolf_and_Cub',\n",
    "        'https://en.wikipedia.org/wiki/Louis_Riel_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/MIND_MGMT',\n",
    "        'https://en.wikipedia.org/wiki/Marvels',\n",
    "        'https://en.wikipedia.org/wiki/Maus',\n",
    "        'https://en.wikipedia.org/wiki/Palestine_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Persepolis_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Powers_(comics),\n",
    "        'https://en.wikipedia.org/wiki/Y:_The_Last_Man',\n",
    "        'https://en.wikipedia.org/wiki/Saga_(comic_book)',\n",
    "        'https://en.wikipedia.org/wiki/Scalped',\n",
    "        'https://en.wikipedia.org/wiki/Scott_Pilgrim',\n",
    "        'https://en.wikipedia.org/wiki/Sin_City',\n",
    "        'https://en.wikipedia.org/wiki/Superman:_Red_Son',\n",
    "        'https://en.wikipedia.org/wiki/Swamp_Thing',\n",
    "        'https://en.wikipedia.org/wiki/The_Authority',\n",
    "        'https://en.wikipedia.org/wiki/The_Dark_Knight_Returns',\n",
    "        'https://en.wikipedia.org/wiki/The_Dark_Phoenix_Saga',\n",
    "        'https://en.wikipedia.org/wiki/The_Galactus_Trilogy',\n",
    "        'https://en.wikipedia.org/wiki/The_Invisibles',\n",
    "        'https://en.wikipedia.org/wiki/The_League_of_Extraordinary_Gentlemen_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/The_Maxx',\n",
    "        'https://en.wikipedia.org/wiki/The_New_Avengers_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/The_Night_Gwen_Stacy_Died',\n",
    "        'https://en.wikipedia.org/wiki/The_Sandman_(Vertigo)',\n",
    "        'https://en.wikipedia.org/wiki/The_Ultimates_(comic_book)',\n",
    "        'https://en.wikipedia.org/wiki/The_Walking_Dead_(comic_book)',\n",
    "        'https://en.wikipedia.org/wiki/Time_(xkcd)',\n",
    "        'https://en.wikipedia.org/wiki/Transmetropolitan',\n",
    "        'https://en.wikipedia.org/wiki/Uncanny_X-Men',\n",
    "        'https://en.wikipedia.org/wiki/V_for_Vendetta',\n",
    "        'https://en.wikipedia.org/wiki/Wanted_(comics)',\n",
    "        'https://en.wikipedia.org/wiki/Watchmen',\n",
    "        ]\n",
    "\n",
    "links, comic_text = get_text(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(list_o_text):\n",
    "    docs = [''.join([char if char not in punctuation else ' ' for char in \n",
    "                     comic]) for comic in list_o_text]\n",
    "\n",
    "    # remove punctuation from string\n",
    "    docs = [word_tokenize(comic) for comic in docs]\n",
    "    # make string into list of words\n",
    "\n",
    "    # 3. Strip out stop words from each tokenized document.\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update(punctuation)\n",
    "    other_words = ['cite', 'cite_note', 'cite_ref', 'class', 'href', 'id', \n",
    "                   'redirect', 'ref', 'refer', 'span', 'sup', 'title', 'wiki']\n",
    "    stop.update(other_words)\n",
    "    docs = [[word for word in words if word.strip(punctuation) not in stop] \n",
    "            for words in docs]\n",
    "    # remove stop words\n",
    "    \n",
    "    # Stemming / Lemmatization\n",
    "    # 1. Stem using both stemmers and the lemmatizer\n",
    "    #porter = PorterStemmer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "    #wordnet = WordNetLemmatizer()\n",
    "    #docs_porter = [[porter.stem(word) for word in words] for words in docs]\n",
    "    docs_snowball = [[snowball.stem(word) for word in words] for words in docs]\n",
    "    #docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "    docs = [' '.join(doc) for doc in docs_snowball]\n",
    "    # for each document, it becomes a long string\n",
    "    return docs\n",
    "\n",
    "docs = clean_text(comic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidfed_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "# docs must be list of strings\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_similarities = cosine_similarity(tfidfed_matrix.todense(), \n",
    "                                        tfidfed_matrix.todense())\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    for j, link in enumerate(links):\n",
    "        print i, j, cosine_similarities[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print cosine_similarities.shape\n",
    "print len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('links.pkl', 'w') as f:\n",
    "    pickle.dump(links, f)\n",
    "with open('vectorizer.pkl', 'w') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "with open('matrix.pkl', 'w') as f:\n",
    "    pickle.dump(tfidfed_matrix, f)\n",
    "with open('comic_text.pkl', 'w') as f:\n",
    "    pickle.dump(comic_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data    \n",
    "#import glob\n",
    "#print glob.glob(\"*.pkl\")\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('links.pkl') as f:\n",
    "    links = pickle.load(f)\n",
    "with open('vectorizer.pkl') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "with open('matrix.pkl') as f:\n",
    "    tfidfed_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import same as above\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize input string > outputs a list of 1 string\n",
    "\n",
    "# import stuff from above\n",
    "def string_cleaner(input_string):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update(punctuation)\n",
    "\n",
    "    user_input = word_tokenize(input_string)\n",
    "    snowball = SnowballStemmer('english')\n",
    "    user_snowball = [snowball.stem(word) for word in user_input if word\n",
    "                     not in stop]\n",
    "    # remove useless words\n",
    "    #lowercase words, keeps only root of words\n",
    "    user = [str(' '.join(user_snowball))]\n",
    "    # converts list of words into string\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# recommendation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cos_sim_recommender(input_string, tfidfed_matrix, links):\n",
    "    user = string_cleaner(input_string)\n",
    "    recommend = cosine_similarity(tfidf_vectorizer.transform(user).todense(), \n",
    "                                  tfidfed_matrix.todense())\n",
    "    # x-axis is the original data, y-axis is the query (raw_input) you put in\n",
    "    # docs must be list of strings\n",
    "    title_index = np.argmax(recommend)\n",
    "    # find max similarity\n",
    "    return links[title_index].split('/')[-1]\n",
    "    # recommendation!\n",
    "\n",
    "\"\"\"\n",
    "new_docs = ['Saga has two planets where poorer and richer fighter with smelly babies',\n",
    "           'watchmen has heroes with silky spectres and masked man',\n",
    "           'vendettta vengeance bombing evey in London',\n",
    "           \"Four months later, V breaks into Jordan Tower, the home of Norsefire's propaganda department\"]\n",
    "\"\"\"\n",
    "cos_sim_recommender(raw_input('type what you want> '), tfidfed_matrix, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Apply k-means clustering to the articles.pkl\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def make_kclusters(tfidf_vectorizer, n_clusters = 8):\n",
    "    #X is matrix\n",
    "    # features is list of words\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(tfidfed_matrix)\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # 2. Print out the centroids.\n",
    "    #print \"cluster centers:\"\n",
    "    #print kmeans.cluster_centers_\n",
    "\n",
    "    # 3. Find the top 10 features for each cluster.\n",
    "    top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-19:-1]\n",
    "    print \"top features for each cluster:\"\n",
    "    for num, centroid in enumerate(top_centroids):\n",
    "        print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))\n",
    "    return kmeans\n",
    "    \n",
    "kmeans = make_kclusters(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print KMean Clusters\n",
    "import numpy as np\n",
    "def print_kclusters(kmeans = kmeans):\n",
    "    titles = np.array([link.split('/')[-1] for link in links])\n",
    "    for index_num, label in enumerate(set(kmeans.labels_)): #index_num isn't true label\n",
    "        indices = np.where(kmeans.labels_ == label)[0]\n",
    "        print index_num\n",
    "        for index in indices:\n",
    "            print titles[index]\n",
    "        print \"\"\n",
    "print_kclusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical clustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_dendrogram(tfidf_matrix):\n",
    "#    distxy = squareform(pdist(tfidf_matrix.todense(), \n",
    "#                              metric='cosine'))\n",
    "    link = linkage(tfidf_matrix.todense(), method='complete', metric='cosine')\n",
    "    dendro = dendrogram(link, color_threshold=1.5, leaf_font_size=9, labels=\n",
    "                    [link.split('/')[-1] for link in links], leaf_rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "draw_dendrogram(tfidfed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get from 1 known comic book to a recommendation\n",
    "\n",
    "#print distxy[9, 0] # numbers Bone > Saga\n",
    "#print distxy[9, 4] # dendro Bone > Time\n",
    "#print distxy[9,:]\n",
    "\n",
    "#print distxy[0, 2] # distxy: Saga > Watchmen\n",
    "#print distxy[0, 22] # dendro: Saga > Maus\n",
    "#print distxy[0]\n",
    "\n",
    "# comic to comic\n",
    "def cos_sim_c2c(input_string, rejected_comics=[], how_many = 3):\n",
    "    titles = np.array([link.split('/')[-1] for link in links])\n",
    "    try:\n",
    "        which_comic = np.where(titles == input_string)[0][0]\n",
    "    except:\n",
    "        return 'Your preferred comic title is not in this database'\n",
    "    distxy = squareform(pdist(tfidfed_matrix.todense(), metric='cosine'))\n",
    "    closest_comics = titles[np.argsort(distxy[which_comic])][1:]\n",
    "    best_n_comics = []\n",
    "    for comic in closest_comics:\n",
    "        if comic in rejected_comics:\n",
    "            continue\n",
    "        else:\n",
    "            best_n_comics.append(comic)\n",
    "        if len(best_n_comics) == how_many:\n",
    "            return best_n_comics\n",
    "    return best_n_comics\n",
    "\n",
    "\n",
    "good_comic = raw_input('What comic do you want a similar one? ')\n",
    "bad_comics = raw_input('Comics you hate? Separate by commas ').split(',')\n",
    "bad_comics = [comic.strip() for comic in bad_comics]\n",
    "cos_sim_c2c(good_comic, bad_comics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maus > Saga_(comic_book),  Watchmen,  The_Sandman_(Vertigo)\n",
    "#distxy = squareform(pdist(tfidfed_matrix.todense(), metric='cosine'))\n",
    "#np.array([link.split('/')[-1] for link in links])[\n",
    "#        np.argsort(distxy[22])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# cosine similarity random comic to comic\n",
    "def cos_sim_rc2c():\n",
    "    titles = np.array([link.split('/')[-1] for link in links])\n",
    "    random_comic = random.choice(titles)\n",
    "    over_recommended_comics = 'The_Sandman_(Vertigo), Watchmen, Saga_(comic_book)'\n",
    "    output = \"A random comic: \" + random_comic + \"; Similar comics: \"\n",
    "    return output + ', '.join(cos_sim_c2c(random_comic, '', how_many = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cos_sim_rc2c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=10)\n",
    "W_sklearn = nmf.fit_transform(tfidfed_matrix)\n",
    "H_sklearn = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reconst_mse(target, left, right):\n",
    "    return (np.array(target - left.dot(right))**2).mean()\n",
    "\n",
    "def describe_nmf_results(document_term_mat = tfidfed_matrix, W = W_sklearn, \n",
    "                 H = H_sklearn, n_top_words = 15, vectorizer = tfidf_vectorizer):\n",
    "    feature_words = vectorizer.get_feature_names()\n",
    "    print(\"Reconstruction error: %f\") %(reconst_mse(document_term_mat, W, H))\n",
    "    for topic_num, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % topic_num)\n",
    "        print(\" \".join([feature_words[i] \\\n",
    "                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    return \n",
    "\n",
    "describe_nmf_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print W_sklearn.shape, H_sklearn.shape, tfidfed_matrix.shape\n",
    "#np.where(titles == 'The_Galactus_Trilogy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_nmf_clusters(W_sklearn = W_sklearn, H_sklearn = H_sklearn):\n",
    "    for cluster_index in range(len(H_sklearn)):\n",
    "        titles = np.array([link.split('/')[-1] for link in links])\n",
    "        comics_in_cluster = []\n",
    "        print cluster_index\n",
    "        for ith, comic in enumerate(W_sklearn):\n",
    "            if cluster_index == np.argmax(comic):\n",
    "                print titles[ith]\n",
    "        print \"\"\n",
    "print_nmf_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_index_in_vectorizer(lst_o_words):\n",
    "    word_indices = []\n",
    "    for word in lst_o_words[0].split():\n",
    "        try:\n",
    "            word_indices.append(tfidf_vectorizer.vocabulary_[word])\n",
    "        except:\n",
    "            continue\n",
    "    return word_indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input string > comic book recommendations; like cosine similarity\n",
    "def nmf_recommender_1(input_string, tfidfed_matrix, links):\n",
    "    user = string_cleaner(input_string)\n",
    "    # get tokenized words from input string\n",
    "    word_indices = word_to_index_in_vectorizer(user)\n",
    "    # for each word, get the index from vectorizer\n",
    "    average_topics = [0] * H_sklearn.shape[0]\n",
    "    for index in range(len(average_topics)):\n",
    "        average_topics[index] = H_sklearn[index][word_indices].mean()\n",
    "    # for each word, get the \"average\" that the word would appear in\n",
    "    guess = np.argmax(cosine_similarity(average_topics, W_sklearn))\n",
    "    return np.array([link.split('/')[-1] for link in links])[guess]\n",
    "    \n",
    "nmf_recommender_1(raw_input('type what you want> '), tfidfed_matrix, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print W_sklearn.shape\n",
    "#print np.array(average_topics).reshape((1, -1)).shape\n",
    "#print np.argmax(cosine_similarity(np.array(average_topics).reshape((1, -1)), \n",
    "#                                  W_sklearn))\n",
    "\n",
    "#print tfidf_vectorizer.transform(user).shape\n",
    "#print tfidfed_matrix.shape\n",
    "#print cosine_similarity(tfidf_vectorizer.transform(user).todense(), \n",
    "#                                  tfidfed_matrix.todense()).shape\n",
    "\n",
    "#W_sklearn[:, index].copy().argsort()[::-1].argsort()\n",
    "#np.argmin(np.mean(np.array(ranked_comics), axis=0))\n",
    "#from collections import Counter\n",
    "#Counter(top_topics)\n",
    "\n",
    "#np.linalg.lstsq(H_sklearn.T, tfidf_vectorizer.transform(user).todense().reshape((-1, 1)))[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def nmf_recommender_2(input_string, tfidfed_matrix=tfidfed_matrix, links=links):\n",
    "    user = string_cleaner(input_string)\n",
    "    # get tokenized words from input string\n",
    "    word_indices = word_to_index_in_vectorizer(user)\n",
    "    # for each word, get the index from vectorizer    \n",
    "    \n",
    "    top_topics = []\n",
    "    for index in word_indices:\n",
    "        top_topics.append(np.argmax(H_sklearn[:, index]))\n",
    "    # get top topics for each word\n",
    "    \n",
    "    ranked_comics = []\n",
    "    for index in top_topics:\n",
    "        order = W_sklearn[:, index].copy().argsort()[::-1]\n",
    "        ranks = order.argsort()\n",
    "        ranked_comics.append(list(ranks))\n",
    "    # rank all comic books for each topic; lower is better\n",
    "    \n",
    "    guess = np.argmin(np.mean(np.array(ranked_comics), axis=0))\n",
    "    # find the comic that is relatively low (closest) on each topic\n",
    "    return np.array([link.split('/')[-1] for link in links])[guess]\n",
    "nmf_recommender_2(raw_input('type in for nmf recommendation> '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similar_comics_nmf(W_sklearn, which_comic):\n",
    "    order_topics_for_comics = get_sorted_topics_matrix(W_sklearn)\n",
    "    this_comic_topics_index = list(W_sklearn[which_comic].copy().argsort()[::-1])\n",
    "    # sorted importance for this comic\n",
    "    possible_options = [np.array(range(len(order_topics_for_comics)))]\n",
    "    # generates which comics are similar by FP growth\n",
    "    for i, topic_index in enumerate(this_comic_topics_index):\n",
    "        where = np.where(order_topics_for_comics[possible_options[-1]][:, i] == topic_index)\n",
    "        possible_options.append(possible_options[-1][where[0]])\n",
    "        if len(possible_options) == 2:\n",
    "            return list(possible_options[-1])\n",
    "        elif len(possible_options) == 1:\n",
    "            return list(possible_options[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates array where it shows each row is each comics sorted topics in descreasing importance\n",
    "def get_sorted_topics_matrix(W_sklearn):\n",
    "    order_topics_for_comics = []\n",
    "    for comic in range(len(W_sklearn)):\n",
    "        topics_for_comic = list(W_sklearn[comic].copy().argsort()[::-1])\n",
    "        order_topics_for_comics.append(topics_for_comic)\n",
    "    return np.array(order_topics_for_comics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similar_comics_nmf(W_sklearn, which_comic):\n",
    "    order_topics_for_comics = get_sorted_topics_matrix(W_sklearn)\n",
    "    this_comic_topics_index = list(W_sklearn[which_comic].copy().argsort()[::-1])\n",
    "    # sorted importance for this comic\n",
    "    possible_options = [np.array(range(len(order_topics_for_comics)))]\n",
    "    # generates which comics are similar by FP growth\n",
    "    for i, topic_index in enumerate(this_comic_topics_index):\n",
    "        where = np.where(order_topics_for_comics[possible_options[-1]][:, i] == topic_index)\n",
    "        possible_options.append(possible_options[-1][where[0]])\n",
    "        if len(possible_options) == 2:\n",
    "            return list(possible_options[-1])\n",
    "        elif len(possible_options) == 1:\n",
    "            return list(possible_options[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside cluster nmf comic to comic recommender\n",
    "def nmf_c2c_in(input_string, rejected_comics = [], how_many = 3):\n",
    "    titles = np.array([link.split('/')[-1] for link in links])\n",
    "    try:\n",
    "        which_comic = np.where(titles == input_string)[0][0]\n",
    "    except:\n",
    "        return 'Your preferred comic title is not in this database'\n",
    "\n",
    "    recommendations = [x for x in get_similar_comics_nmf(W_sklearn, which_comic) if x != which_comic]\n",
    "    #recommendations = get_comic_index(W_sklearn, which_comic, float('inf'))\n",
    "    comic_recommendations = titles[np.array(recommendations)]\n",
    "    np.random.shuffle(comic_recommendations)\n",
    "    print comic_recommendations\n",
    "        \n",
    "    best_n_comics = []\n",
    "    for comic in comic_recommendations:\n",
    "        if comic in rejected_comics:\n",
    "            continue\n",
    "        else:\n",
    "            best_n_comics.append(comic)\n",
    "        if len(best_n_comics) == how_many:\n",
    "            return best_n_comics\n",
    "    return best_n_comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_comic = raw_input('What comic do you want a similar one? ')\n",
    "bad_comics = raw_input('Comics you hate? Separate by commas ').split(',')\n",
    "bad_comics = [comic.strip() for comic in bad_comics]\n",
    "\n",
    "nmf_c2c_in(good_comic, bad_comics)\n",
    "\n",
    "# input Scalped\n",
    "# possible recommendations: ['The_Invisibles' 'Girl_Genius' 'From_Hell'\n",
    "# 'List_of_Y:_The_Last_Man_story_arcs' 'Hellboy:_Seed_of_Destruction'\n",
    "# 'Wanted_(comics)' 'V_for_Vendetta' 'List_of_Preacher_story_arcs'\n",
    "# 'List_of_Criminal_story_arcs'\n",
    "# 'The_League_of_Extraordinary_Gentlemen_(comics)']\n",
    "\n",
    "# output\n",
    "\n",
    "# restrict: List_of_Y:_The_Last_Man_story_arcs, From_Hell, Girl_Genius, List_of_Criminal_story_arcs,\n",
    "# Hellboy:_Seed_of_Destruction, The_League_of_Extraordinary_Gentlemen_(comics), Wanted_(comics)\n",
    "\n",
    "# get: V, Preacher, Invisibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
